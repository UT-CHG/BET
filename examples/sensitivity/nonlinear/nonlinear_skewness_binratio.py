# Copyright (C) 2014-2015 The BET Development Team

"""
This example generates uniform random samples in the unit hypercube and
corresponding QoIs (data) generated by a linear map Q.  We then calculate the
gradients using an RBF scheme and use the gradient information to choose the
optimal set of 2 (3, 4, ... Lambda_dim) QoIs to use in the inverse problem.

Every real world problem requires special attention regarding how we choose
*optimal QoIs*.  This set of examples (examples/sensitivity/linear) covers
some of the more common scenarios using easy to understand linear maps.

In this *condnum_binratio* example we choose *optimal QoIs* to be the set of QoIs
of size Lambda_dim that has optimal skewness properties which will yield an
inverse solution that can be approximated well.  The uncertainty in our data is
relative to the range of data measured in each QoI (bin_ratio).
"""

import numpy as np
import bet.sensitivity.gradients as grad
import bet.sensitivity.chooseQoIs as cQoI
import bet.calculateP.simpleFunP as simpleFunP
import bet.calculateP.calculateP as calculateP
import bet.postProcess.postTools as postTools
import bet.Comm as comm
import itertools
from scipy.special import comb
import matplotlib.pyplot as plt
from itertools import combinations
from pylab import *
# Let Lambda be a 5 dimensional hypercube
Lambda_dim = 2
Data_dim = 3
num_samples = 1E5
num_centers = 100 # at how many points  do we compute gradient information?

np.random.seed(0)

#####

Lambda_min = 0.0 # unused right now. - implementinto random samples.
Lambda_max = 1.0

# what do these two lines do? - computes n choose k.
combs = int(comb(Data_dim, Lambda_dim))
combs_array = np.array(list(combinations(range(10),2)))

rand_int = np.int(np.round(np.random.random(1) * 1000))

r = [[0.19, -0.25, 0.09, -0.86, -0.07, 0.29], \
    [0.80, 0.47, 0.08, -0.64,  -0.98,  -1.0], \
    [-0.49, -0.44, -0.21, 0.67,  -0.10, 1.2]]
def Q(x):
    np.random.seed(rand_int)
    q = np.zeros([x.shape[0], Data_dim])
    for i in range(Data_dim):
        # rand_vec = 2 * np.random.random(6) - 1
        # q[:, i] = rand_vec[0] * x[:, 0]**5 + rand_vec[1] * x[:, 1]**3 + \
        #     rand_vec[2] * x[:, 0] **3 * x[:, 1] + rand_vec[3] * x[:, 0] + \
        #     rand_vec[4] * x[:, 1] + rand_vec[5]
        q[:, i] = r[i][0] * x[:, 0]**5 + r[i][1] * x[:, 1]**3 + \
            r[i][2] * x[:, 0] **3 * x[:, 1] + r[i][3] * x[:, 0] + \
            r[i][4] * x[:, 1] + r[i][5]
    np.random.seed(None)
    return q

np.random.seed(0)
samples = np.random.random([num_samples, Lambda_dim])
data = Q(samples)
# Let the map Q be a random matrix of size (Data_dim, Lambda_dim)

#####



# Calculate the gradient vectors at some subset of the samples.  Here the
# *normalize* argument is set to *True* because we are using bin_ratio to
# determine the uncertainty in our data.
G = grad.calculate_gradients_rbf(samples, data, centers=samples[:num_centers, :],
    normalize=True)

# With these gradient vectors, we are now ready to choose an optimal set of
# QoIs to use in the inverse problem, based on optimal skewness properites of
# QoI vectors.  The most robust method for this is
# :meth:~bet.sensitivity.chooseQoIs.chooseOptQoIs_large which returns the
# best set of 2, 3, 4 ... until Lambda_dim.  This method returns a list of
# matrices.  Each matrix has 10 rows, the first column representing the
# average condition number of the Jacobian of Q, and the rest of the columns
# the corresponding QoI indices.
best_sets = cQoI.chooseOptQoIs_large(G, volume=False)

###############################################################################

# At this point we have determined the optimal set of QoIs to use in the inverse
# problem.  Now we compare the support of the inverse solution using
# different sets of these QoIs.  We set Q_ref to correspond to the center of
# the parameter space.  We choose the set of QoIs to consider.

QoI_indices = [1, 2] # choose up to Data_dim
#QoI_indices = [3, 6]
#QoI_indices = [0, 3]
#QoI_indices = [3, 5, 6, 8, 9]
#QoI_indices = [0, 3, 5, 8, 9]
#QoI_indices = [3, 4, 5, 8, 9]
#QoI_indices = [2, 3, 5, 6, 9]

# Restrict the data to have just QoI_indices
data = data[:, QoI_indices]
# Q_ref = Q[QoI_indices, :].dot(0.5 * np.ones(Lambda_dim)) # linear case - center of data space.
ref_lambda  = [0.5, 0.5]
Q_ref = Q(np.array([ref_lambda]))[0][QoI_indices]
# bin_ratio defines the uncertainty in our data
bin_ratio = 0.25

# Find the simple function approximation
(d_distr_prob, d_distr_samples, d_Tree) = simpleFunP.uniform_hyperrectangle(\
    data=data, Q_ref=Q_ref, bin_ratio=bin_ratio, center_pts_per_edge = 1)

# Calculate probablities making the Monte Carlo assumption
(P,  lam_vol, io_ptr) = calculateP.prob(samples=samples, data=data,
    rho_D_M=d_distr_prob, d_distr_samples=d_distr_samples)

percentile = 1.0
# Sort samples by highest probability density and find how many samples lie in
# the support of the inverse solution.  With the Monte Carlo assumption, this
# also tells us the approximate volume of this support.
(num_samples, P_high, samples_high, lam_vol_high, data_high, sort) =\
    postTools.sample_highest_prob(top_percentile=percentile, P_samples=P,
    samples=samples, lam_vol=lam_vol,data = data,sort=True)

# Print the number of samples that make up the highest percentile percent
# samples and ratio of the volume of the parameter domain they take up
print '\n'
if comm.rank == 0:
    print (num_samples, np.sum(lam_vol_high))
