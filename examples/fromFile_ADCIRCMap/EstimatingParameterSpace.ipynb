{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Estimate the parameter space probabiliy density with a 1D data space\n",
    "\n",
    "([From BET Documentation](http://ut-chg.github.io/BET/examples/example_rst_files/Q_1D.html#q1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the parameter space $\\Lambda \\subset \\mathbb{R}^2$ is 2 dimensional. This example demostrates three different methods to estimate $\\hat{\\rho}_{\\Lambda, j}$ where\n",
    "$$P_\\Lambda \\approx \\sum_{\\mathcal{V}_j \\subset A} \\hat{\\rho}_{\\Lambda, j}.$$\n",
    "\n",
    "These methods are distinguished primarily by the way $\\mathcal{V}_j$ are defined and the approximation of the volume of $\\mathcal{V}_j$. See [Q_1D.py](Q_1D.py) for the example source code.\n",
    "\n",
    "First, import the necessary packages and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bet.sampling.basicSampling as bsam\n",
    "import bet.calculateP.calculateP as calcP\n",
    "import bet.calculateP.simpleFunP as sfun\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import bet.sample as sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data where our parameter space is 2-dimensional and load a reference solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import \"Truth\" (reference solution)\n",
    "mdat = sio.loadmat('../matfiles/Q_2D')\n",
    "Q = mdat['Q']\n",
    "Q_ref = mdat['Q_true']\n",
    "\n",
    "# Import Data \n",
    "points = mdat['points']\n",
    "lam_domain = np.array([[0.07, .15], [0.1, 0.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the to the `points`, $\\lambda_{samples} = \\{ \\lambda^{(j) } \\}, j = 1, \\ldots, N$, to create an input `sample_set` object. These `points` are the points in parameter space where we solve the forward model to generate the data `Q` where $Q_j = Q(\\lambda^{(j)})$.\n",
    "\n",
    "Define the parameter domain $\\Lambda$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lam_domain = np.array([[0.07, .15], [0.1, 0.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input sample set objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sample_set = sample.sample_set(points.shape[0])\n",
    "input_sample_set.set_values(points.transpose())\n",
    "input_sample_set.set_domain(lam_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for approximating $\\hat{\\rho}_{\\Lambda, j}$\n",
    "For ease of use we have created a function, `postprocess(station_nums, ref_num)` so that we can loop through different QoI (maximum water surface height at various measurement stations) and reference solutions (point in data space around which we center a uniform probability solution. The three methods for approximating $\\hat{\\rho}_{\\Lambda, j}$ are combined in the postprocessing function. \n",
    "\n",
    "The function is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def postprocess(station_nums, ref_num):\n",
    "```\n",
    "\n",
    "Define the filename to save $\\hat{\\rho}_{\\Lambda, j}$ to:\n",
    "\n",
    "```python\n",
    "    filename = 'P_q'+str(station_nums[0]+1)+'_q'\n",
    "    if len(station_nums) == 3:\n",
    "        filename += '_q'+str(station_nums[2]+1)\n",
    "    filename += '_truth_'+str(ref_num+1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data space $\\mathcal{D} \\subset \\mathbb{R}^d$ where $d$ is the dimension of the data space:\n",
    "\n",
    "```python\n",
    "    data = Q[:, station_nums]\n",
    "    output_sample_set = sample.sample_set(data.shape[1])\n",
    "    output_sample_set.set_values(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the refernce solution. We define a region of interest, $R_{ref} \\subset \\mathcal{D}$ centered at $Q_{ref}$ that is 15% the length of $q_n$ (the QoI for station $n$). We set $\\rho_\\mathcal{D}(q) = \\dfrac{\\mathbf{1}_{R_{ref}}(q)}{||\\mathbf{1}_{R_{ref}}||}$ and then create a simple function approximation to this density:\n",
    "\n",
    "```python\n",
    "    q_ref = Q_ref[ref_num, station_nums]\n",
    "    output_probability_set = sfun.regular_partition_uniform_distribution_rectangle_scaled(\\\n",
    "            output_sample_set, q_ref, rect_scale=0.15,\n",
    "            cells_per_dimension=np.ones((data.shape[1],)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 1e6 uniformly distributed points in $\\Lambda$. We call these points $\\lambda_{emulate} = \\{ \\lambda_j \\}_{j=1}^{10^6}$:\n",
    "\n",
    "```python\n",
    "    num_l_emulate = 1e4\n",
    "    set_emulated = bsam.random_sample_set('r', lam_domain, num_l_emulate)\n",
    "    my_disc = sample.discretization(input_sample_set, output_sample_set,\n",
    "            output_probability_set, emulated_input_sample_set=set_emulated)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimation Method 1:** Calculate $\\hat{\\rho}_{\\Lambda, j}$ where $\\mathcal{V}_j$ are the voronoi cells defined by $\\lambda_{emulate}$:\n",
    "\n",
    "```python\n",
    "    calcP.prob_on_emulated_samples(my_disc)\n",
    "    sample.save_discretization(my_disc, filename, \"prob_on_emulated_samples_solution\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimation Method 2:** Calculate $\\hat{\\rho}_{\\Lambda, j}$ where $\\mathcal{V}_j$ are the voronoi cells defined by $\\lambda_{samples}$ assume that $\\lambda_{samples}$ are uniformly distributed and therefore have approximately the same volume:\n",
    "\n",
    "```python\n",
    "    input_sample_set.estimate_volume_mc()\n",
    "    calcP.prob(my_disc)\n",
    "    sample.save_discretization(my_disc, filename, \"prob_solution\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimation Method 3:** Calculate $\\hat{\\rho}_{\\Lambda, j}$ where $\\mathcal{V}_j$ are the voronoi cells defined by $\\lambda_{samples}$ and we approximate the volume of $\\mathcal{V}_j$ using Monte Carlo integration. We use $\\lambda_{emulate}$ to estimate the volume of $\\mathcal{V}_j$.\n",
    "\n",
    "```python\n",
    "    calcP.prob_with_emulated_volumes(my_disc)\n",
    "    sample.save_discretization(my_disc, filename, \"prob_with_emulated_volumes_solution\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the above pieces together, the function `postprocess(station_nums, ref_num)` will be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess(station_nums, ref_num):\n",
    "    \n",
    "    filename = 'P_q'+str(station_nums[0]+1)+'_q'\n",
    "    if len(station_nums) == 3:\n",
    "        filename += '_q'+str(station_nums[2]+1)\n",
    "    filename += '_ref_'+str(ref_num+1)\n",
    "\n",
    "    data = Q[:, station_nums]\n",
    "    output_sample_set = sample.sample_set(data.shape[1])\n",
    "    output_sample_set.set_values(data)\n",
    "    q_ref = Q_ref[ref_num, station_nums]\n",
    "\n",
    "    # Create Simple function approximation\n",
    "    # Save points used to parition D for simple function approximation and the\n",
    "    # approximation itself (this can be used to make close comparisions...)\n",
    "    output_probability_set = sfun.regular_partition_uniform_distribution_rectangle_scaled(\\\n",
    "            output_sample_set, q_ref, rect_scale=0.15,\n",
    "            cells_per_dimension=np.ones((data.shape[1],)))\n",
    "\n",
    "    num_l_emulate = 1e4\n",
    "    set_emulated = bsam.random_sample_set('r', lam_domain, num_l_emulate)\n",
    "    my_disc = sample.discretization(input_sample_set, output_sample_set,\n",
    "            output_probability_set, emulated_input_sample_set=set_emulated)\n",
    "\n",
    "    print \"Finished emulating lambda samples\"\n",
    "\n",
    "    # Calculate P on lambda emulate\n",
    "    print \"Calculating prob_on_emulated_samples\"\n",
    "    calcP.prob_on_emulated_samples(my_disc)\n",
    "    sample.save_discretization(my_disc, filename, \"prob_on_emulated_samples_solution\")\n",
    "\n",
    "    # Calclate P on the actual samples with assumption that voronoi cells have\n",
    "    # equal size\n",
    "    input_sample_set.estimate_volume_mc()\n",
    "    print \"Calculating prob\"\n",
    "    calcP.prob(my_disc)\n",
    "    sample.save_discretization(my_disc, filename, \"prob_solution\")\n",
    "\n",
    "    # Calculate P on the actual samples estimating voronoi cell volume with MC\n",
    "    # integration\n",
    "    calcP.prob_with_emulated_volumes(my_disc)\n",
    "    print \"Calculating prob_with_emulated_volumes\"\n",
    "    sample.save_discretization(my_disc, filename, \"prob_with_emulated_volumes_solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, having defined our postprocessing function, we calculate $\\hat{\\rho}_{\\Lambda, j}$ for three reference solutions and 3 QoI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n"
     ]
    }
   ],
   "source": [
    "ref_nums = [6, 11, 15] # 7, 12, 16\n",
    "stations = [1, 4, 5] # 2, 5, 6\n",
    "\n",
    "ref_nums, stations = np.meshgrid(ref_nums, stations)\n",
    "ref_nums = ref_nums.ravel()\n",
    "stations = stations.ravel()\n",
    "\n",
    "for tnum, stat in zip(ref_nums, stations):\n",
    "    postprocess([0], tnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Estimate the parameter space probabiliy density with a 2D data space\n",
    "([From BET Documentation](http://ut-chg.github.io/BET/examples/example_rst_files/Q_2D.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the parameter space $\\Lambda \\subset \\mathbb{R}^2$ is 2 dimensional. This example demostrates three different methods to estimate $\\hat{\\rho}_{\\Lambda, j}$ where\n",
    "$$P_\\Lambda \\approx \\sum_{\\mathcal{V}_j \\subset A} \\hat{\\rho}_{\\Lambda, j}.$$\n",
    "\n",
    "These methods are distinguished primarily by the way $\\mathcal{V}_j$ are defined and the approximation of the volume of $\\mathcal{V}_j$. See [Q_2D.py](Q_2D.py) for the example source code. Since this example is essentially the same as the previous example in this notebook that estimates the parameter space probabiliy density with a 1D data space we will only highlight the differences between the two.\n",
    "\n",
    ">**Note:** *If the code from the previous example above has already been run, then the majority of environment has already been defined and the following code excerpts can be run as written.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, redefine the input sample set, here it is 2D rather than 1D:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import \"Truth\"\n",
    "mdat = sio.loadmat('../matfiles/Q_2D')\n",
    "Q = mdat['Q']\n",
    "Q_ref = mdat['Q_true']\n",
    "\n",
    "# Import Data\n",
    "points = mdat['points']\n",
    "lam_domain = np.array([[0.07, .15], [0.1, 0.2]]) # Note this is now 2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sample_set = sample.sample_set(points.shape[0])\n",
    "input_sample_set.set_values(points.transpose())\n",
    "input_sample_set.set_domain(lam_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the postprocessing function, `postprocess(station_nums, ref_num)`, defined earlier in the following ways.\n",
    "\n",
    "First, change the save filename for the estimates of $\\hat{\\rho}_{\\Lambda, j}$:\n",
    "\n",
    "```python\n",
    "    filename = 'P_q'+str(station_nums[0]+1)+'_q'+str(station_nums[1]+1)\n",
    "    if len(station_nums) == 3:\n",
    "        filename += '_q'+str(station_nums[2]+1)\n",
    "    filename += '_truth_'+str(ref_num+1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data space $\\mathcal{D} \\subset \\mathbb{R}^d$ where $d$ is the dimension of the data space:\n",
    "\n",
    "```python\n",
    "    data = Q[:, station_nums]\n",
    "    output_sample_set = sample.sample_set(data.shape[1])\n",
    "    output_sample_set.set_values(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the refernce solution. We define a region of interest, $R_{ref} \\subset \\mathcal{D}$ centered at $Q_{ref}$ with sides 15% the length of $q_{station\\_num[0]}$ and $q_{station\\_num[1]}$ (the QoI for stations $n$). We set $\\rho_\\mathcal{D}(q) = \\dfrac{\\mathbf{1}_{R_{ref}}(q)}{||\\mathbf{1}_{R_{ref}}||}$ and then create a simple function approximation to this density:\n",
    "\n",
    "```python\n",
    "    q_ref = Q_ref[ref_num, station_nums]\n",
    "\n",
    "    output_probability_set = sfun.regular_partition_uniform_distribution_rectangle_scaled(\\\n",
    "            output_sample_set, q_ref, rect_scale=0.15,\n",
    "            cells_per_dimension=np.ones((data.shape[1],)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, the postprocessing function, `postprocess(station_nums, ref_num)`, will estimate the parameter $\\hat{\\rho}_{\\Lambda, j}$ using the three different methods discussed earlier. The modified `postprocess(station_nums, ref_num)` function is shown in its entirety below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess(station_nums, ref_num):\n",
    "    \n",
    "    filename = 'P_q'+str(station_nums[0]+1)+'_q'+str(station_nums[1]+1)\n",
    "    if len(station_nums) == 3:\n",
    "        filename += '_q'+str(station_nums[2]+1)\n",
    "    filename += '_ref_'+str(ref_num+1)\n",
    "\n",
    "    data = Q[:, station_nums]\n",
    "    output_sample_set = sample.sample_set(data.shape[1])\n",
    "    output_sample_set.set_values(data)\n",
    "    q_ref = Q_ref[ref_num, station_nums]\n",
    "\n",
    "    # Create Simple function approximation\n",
    "    # Save points used to parition D for simple function approximation and the\n",
    "    # approximation itself (this can be used to make close comparisions...)\n",
    "    output_probability_set = sfun.regular_partition_uniform_distribution_rectangle_scaled(\\\n",
    "            output_sample_set, q_ref, rect_scale=0.15,\n",
    "            cells_per_dimension=np.ones((data.shape[1],)))\n",
    "\n",
    "    num_l_emulate = 1e4\n",
    "    set_emulated = bsam.random_sample_set('r', lam_domain, num_l_emulate)\n",
    "    my_disc = sample.discretization(input_sample_set, output_sample_set,\n",
    "            output_probability_set, emulated_input_sample_set=set_emulated)\n",
    "\n",
    "    print \"Finished emulating lambda samples\"\n",
    "\n",
    "    # Calculate P on lambda emulate\n",
    "    print \"Calculating prob_on_emulated_samples\"\n",
    "    calcP.prob_on_emulated_samples(my_disc)\n",
    "    sample.save_discretization(my_disc, filename, \"prob_on_emulated_samples_solution\")\n",
    "\n",
    "    # Calclate P on the actual samples with assumption that voronoi cells have\n",
    "    # equal size\n",
    "    input_sample_set.estimate_volume_mc()\n",
    "    print \"Calculating prob\"\n",
    "    calcP.prob(my_disc)\n",
    "    sample.save_discretization(my_disc, filename, \"prob_solution\")\n",
    "\n",
    "    # Calculate P on the actual samples estimating voronoi cell volume with MC\n",
    "    # integration\n",
    "    calcP.prob_with_emulated_volumes(my_disc)\n",
    "    print \"Calculating prob_with_emulated_volumes\"\n",
    "    sample.save_discretization(my_disc, filename, \"prob_with_emulated_volumes_solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate $\\hat{\\rho}_{\\Lambda, j}$ for three reference solutions and the QoI $( (q_1,q_2), (q_1, q_5)$, and $(q_1, q_6))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n",
      "Finished emulating lambda samples\n",
      "Calculating prob_on_emulated_samples\n",
      "Calculating prob\n",
      "Calculating prob_with_emulated_volumes\n"
     ]
    }
   ],
   "source": [
    "# Post-process and save P and emulated points\n",
    "ref_nums = [6, 11, 15] # 7, 12, 16\n",
    "stations = [1, 4, 5] # 2, 5, 6\n",
    "\n",
    "ref_nums, stations = np.meshgrid(ref_nums, stations)\n",
    "ref_nums = ref_nums.ravel()\n",
    "stations = stations.ravel()\n",
    "\n",
    "for tnum, stat in zip(ref_nums, stations):\n",
    "    postprocess([0, stat], tnum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Estimate the parameter space probabiliy density with a 3D data space\n",
    "\n",
    "([From BET Documentation](http://ut-chg.github.io/BET/examples/example_rst_files/Q_3D.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples the parameter space $\\Lambda \\subset \\mathbb{R}^3$ is 3 dimensional.\n",
    "\n",
    "This example demostrates how to estimate $\\hat{\\rho}_{\\Lambda, j}$ using `prob()` where\n",
    "$$P_\\Lambda \\approx \\sum_{\\mathcal{V}_j \\subset A} \\hat{\\rho}_{\\Lambda, j}.$$\n",
    "\n",
    "See [Q_3D.py](Q_3D.py) for the example source code. Since example is essentially the same as the previous examples in this notebook for estimating the parameter space probabiliy density with a 1D and 2D data spaces, we will only highlight the differences between the two.\n",
    "\n",
    ">**Note:** *If the code from the previous example above has already been run, then the majority of environment has already been defined and the following code excerpts can be run as written.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, instead of loading data for a 2-dimensional parameter space we load data for a 3-dimensional data space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import \"Truth\"\n",
    "mdat = sio.loadmat('../matfiles/Q_3D')\n",
    "Q = mdat['Q']\n",
    "Q_ref = mdat['Q_true']\n",
    "\n",
    "# Import Data\n",
    "samples = mdat['points'].transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the parameter domain $\\Lambda$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lam_domain = np.array([[-900, 1200], [0.07, .15], [0.1, 0.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input sample set, here it is 3D rather than 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create input, output, and discretization from data read from file\n",
    "points = mdat['points']\n",
    "input_sample_set = sample.sample_set(points.shape[0])\n",
    "input_sample_set.set_values(points.transpose())\n",
    "input_sample_set.set_domain(lam_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our postprocessing function, simply change the naming convention for the filename to save $\\hat{\\rho}_{\\Lambda, j}$:\n",
    "\n",
    "```python\n",
    "    filename = 'P_q'+str(station_nums[0]+1)+'_q'+str(station_nums[1]+1)\n",
    "    if len(station_nums) == 3:\n",
    "        filename += '_q'+str(station_nums[2]+1)\n",
    "    filename += '_ref_'+str(ref_num+1)\n",
    "```\n",
    "\n",
    "The edited postprocessing function `postprocess(station_nums, ref_num)` is shown in its entirety below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess(station_nums, ref_num):\n",
    "    \n",
    "    filename = 'P_q'+str(station_nums[0]+1)+'_q'+str(station_nums[1]+1)\n",
    "    if len(station_nums) == 3:\n",
    "        filename += '_q'+str(station_nums[2]+1)\n",
    "    filename += '_ref_'+str(ref_num+1)\n",
    "\n",
    "    data = Q[:, station_nums]\n",
    "    output_sample_set = sample.sample_set(data.shape[1])\n",
    "    output_sample_set.set_values(data)\n",
    "    q_ref = Q_ref[ref_num, station_nums]\n",
    "\n",
    "    # Create Simple function approximation\n",
    "    # Save points used to parition D for simple function approximation and the\n",
    "    # approximation itself (this can be used to make close comparisions...)\n",
    "    output_probability_set = sfun.regular_partition_uniform_distribution_rectangle_scaled(\\\n",
    "            output_sample_set, q_ref, rect_scale=0.15,\n",
    "            cells_per_dimension=np.ones((data.shape[1],)))\n",
    "\n",
    "    my_disc = sample.discretization(input_sample_set, output_sample_set,\n",
    "            output_probability_set)\n",
    "\n",
    "    # Calclate P on the actual samples with assumption that voronoi cells have\n",
    "    # equal size\n",
    "    input_sample_set.estimate_volume_mc()\n",
    "    print \"Calculating prob\"\n",
    "    calcP.prob(my_disc)\n",
    "    sample.save_discretization(my_disc, filename, \"prob_solution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Solutions\n",
    "\n",
    "Finally, we calculate $\\hat{\\rho}_{\\Lambda, j}$ for the 15th reference solution at:\n",
    "\n",
    "* $Q = (q_1, q_5, q_2)$, \n",
    "* $Q=(q_1, q_5)$, \n",
    "* $Q=(q_1, q_5, q_{12})$,\n",
    "* $Q=(q_1, q_9, q_7),$ and \n",
    "* $Q=(q_1, q_9, q_{12})$.\n",
    "\n",
    "Try other reference solutions or other points in $Q$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating prob\n"
     ]
    }
   ],
   "source": [
    "# Post-process and save P and emulated points\n",
    "ref_num = 14 # 15th reference solution\n",
    "#ref_num = 15 # 16th reference solution\n",
    "\n",
    "# q1, q5, q2 \n",
    "station_nums = [0, 4, 1] # 1, 5, 2\n",
    "postprocess(station_nums, ref_num)\n",
    "\n",
    "\n",
    "# q1, q5\n",
    "# station_nums = [0, 4] # 1, 5\n",
    "# postprocess(station_nums, ref_num)\n",
    "\n",
    "# q1, q5, q12\n",
    "#station_nums = [0, 4, 11] # 1, 5, 12\n",
    "#postprocess(station_nums, ref_num)\n",
    "\n",
    "\n",
    "#station_nums = [0, 8, 6] # 1, 9, 7\n",
    "#postprocess(station_nums, ref_num)\n",
    "\n",
    "\n",
    "#station_nums = [0, 8, 11] # 1, 9, 12\n",
    "#postprocess(station_nums, ref_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
