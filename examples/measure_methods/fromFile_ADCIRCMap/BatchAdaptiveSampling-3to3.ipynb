{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Batch Adaptive Sampling (3-to-3 example)\n",
    "([From BET Documentation](http://ut-chg.github.io/BET/examples/example_rst_files/fromfile3D.html#fromfile3dexample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** *This example shows how to generate adaptive samples in a specific way by implicitly defining an input event of interest. It does NOT show how to solve the stochastic inverse problem using these samples, which can be found by reading other examples. Thus, we only present the first few steps involved in discretizing the parameter and data spaces using a specific type of adaptive sampling. The user is referred to some other examples for filling in the remaining steps for solving the stochastic inverse problem following the construction of the adaptive samples.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will walk through the following [example](fromFile3D.py) that uses a linear interpolant of a 3-dimensional QoI map used to define a 3-dimensional data space. The parameter space is also 3-dimensional.\n",
    "\n",
    "This example specifically demonstrates the adaptive generation of samples using a goal-oriented adaptive sampling algorithm. This example is based upon the results shown in Section 8.6 of the manuscript [Definition and solution of a stochastic inverse problem for the Manning’s n parameter field in hydrodynamic models](http://dx.doi.org/10.1016/j.advwatres.2015.01.011) where the QoI map is given by $Q(\\lambda) = (q_1(\\lambda), q_5(\\lambda), q_2(\\lambda))$. We refer the reader to that example for more information about the physical interpretation of the parameter and data space, as well as the physical locations of the observation stations defining the QoI map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** *In this example, we have used ADCIRC to generate data files based on a regular discretization of the parameter space whose sole purpose is to create an (accurate) surrogate QoI map defined as a piecewise linear interpolant. This is quite different from many of the other examples, but the use of the surrogate QoI map is immaterial. The user could also interface the sampler directly to ADCIRC, but this would require a copy of ADCIRC, the finite element mesh, and significant training on the use of this state-of-the-art shallow water equation code. The primary focus of this example is the generation of adaptive samples. If the user knows how to use the ADCIRC model, then the user may instead opt to significantly change Step (1) below to interface to ADCIRC instead of to our “model” defined in terms of the surrogate QoI map. Interfacing to ADCIRC directly would likely require the use of [PolyADCIRC](https://github.com/UT-CHG/PolyADCIRC).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** *This example is very similar to [Example: Batch Adaptive Sampling (2-to-2 example)](BatchAdaptiveSampling-2to2.ipynb) which involved a 2-to-2 map. The user may want to modify either example to involve fewer QoI’s in the map (e.g., defining a 2-to-1 or 3-to-2 or 3-to-1 map). The example discussed in Section 8.6 of the paper referenced above discusses that the results for solving the stochastic inverse problem using a 3-to-3 map are almost identical to those using a 3-to-2 map.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a single set of adaptive samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step (0): Setting up the environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bet.sampling.adaptiveSampling as asam\n",
    "import bet.postProcess.plotDomains as pDom\n",
    "import scipy.io as sio\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step (1): Define the interface to the model and goal-oriented adaptive sampler\n",
    "\n",
    "This is where we interface the adaptive sampler imported above to the model. In other examples, we have imported a Python interface to a computational model. In this example, we instead define the model as a (piecewise-defined) linear interpolant to the QoI map $Q(\\lambda) = (q_1(\\lambda), q_5(\\lambda), q_2(\\lambda))$ using data read from a `.mat` [file](../matfiles/Q_3D.mat):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_nums = [0, 4, 1] # 1, 5, 2\n",
    "mdat = sio.loadmat('../matfiles/Q_3D')\n",
    "Q = mdat['Q']\n",
    "Q = Q[:, station_nums]\n",
    "# Create experiment model\n",
    "points = mdat['points']\n",
    "def model(inputs):\n",
    "    interp_values = np.empty((inputs.shape[0], Q.shape[1]))\n",
    "    for i in xrange(Q.shape[1]):\n",
    "        interp_values[:, i] = griddata(points.transpose(), Q[:, i],\n",
    "            inputs)\n",
    "    return interp_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the adaptive sampler defined by `rhoD_kernel`, which requires an identification of a data distribution used to modify the transition kernel for input samples. The idea is to place more samples in the parameter space that correspond to a contour event of higher probability as specified by the data distribution `rho_D` shown below.\n",
    "\n",
    "First, we create the `transition_set` with an initial step size ratio of 0.5 and a minimum, maximum step size ratio of `.5**5` and 1.0 respectively. Note that this algorithm only generates samples inside the parameter domain, `lam_domain` (see Step (2) below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Transition Kernel\n",
    "transition_set = asam.transition_set(.5, .5**5, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we implicty designate a region of interest $\\Lambda_k = Q^{-1}(D_k)$ in $\\Lambda$ for some $D_k \\subset \\mathcal{D}$ through the use of the data distribution kernel. In this instance we choose our kernel $p_k(Q) = \\rho_\\mathcal{D}(Q)$, see `rhoD_kernel`.\n",
    "\n",
    "We choose some $\\lambda_{ref}$ and let $Q_{ref} = Q(\\lambda_{ref})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_ref = mdat['Q_true']\n",
    "Q_ref = Q_ref[14, station_nums] # 15th/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a 3-D box, $R_{ref} \\subset \\mathcal{D}$ centered at $Q(\\lambda_{ref})$ with sides 15% the length of $q_1$, $q_5$, and $q_2$. Set $\\rho_\\mathcal{D}(q) = \\dfrac{\\mathbf{1}_{R_{ref}}(q)}{||\\mathbf{1}_{R_{ref}}||}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_ratio = 0.15\n",
    "bin_size = (np.max(Q, 0)-np.min(Q, 0))*bin_ratio\n",
    "# Create kernel\n",
    "maximum = 1/np.product(bin_size)\n",
    "def rho_D(outputs):\n",
    "    rho_left = np.repeat([Q_ref-.5*bin_size], outputs.shape[0], 0)\n",
    "    rho_right = np.repeat([Q_ref+.5*bin_size], outputs.shape[0], 0)\n",
    "    rho_left = np.all(np.greater_equal(outputs, rho_left), axis=1)\n",
    "    rho_right = np.all(np.less_equal(outputs, rho_right),axis=1)\n",
    "    inside = np.logical_and(rho_left, rho_right)\n",
    "    max_values = np.repeat(maximum, outputs.shape[0], 0)\n",
    "    return inside.astype('float64')*max_values\n",
    "\n",
    "kernel_rD = asam.rhoD_kernel(maximum, rho_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that when the region of interest has been “found” by some sample in a chain, the transition set is modified by the adaptive sampler (it is made smaller) so that more samples are placed within this event of interest.\n",
    "\n",
    "Given a (M, mdim) data vector `rhoD_kernel` expects that `rho_D` will return a ndarray of shape (M,).\n",
    "\n",
    "Next, we create the `sampler`. This `sampler` will create 80 independent sampling chains that are each 125 samples long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create sampler\n",
    "chain_length = 125\n",
    "num_chains = 80\n",
    "num_samples = chain_length*num_chains\n",
    "sampler = asam.sampler(num_samples, chain_length, model)\n",
    "sample_save_file = 'sandbox3d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    "* *In the first line of code above, change `chain_length` and `num_chains` to reduce the total number of forward solves.*\n",
    "* *If `num_chains = 1` above, then this is no longer a “batch” sampling process where multiple chains are run simultaneously to “search for” the region of interest.*\n",
    "* *Saves to `sandbox3d.mat`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step (2) [and Step (3)]: Describe and (adaptively) sample the input (and output) space\n",
    "The adaptive sampling of the input space requires feedback from the corresponding output samples, so the sets of samples are, in a sense, created simultaneously in order to define the discretization of the spaces used to solve the stochastic inverse problem. While this can always be the case, in other examples, we often sampled the input space completely in one step, and then propagated the samples through the model to generate the QoI samples in another step, and these two samples sets together were used to define the discretization object used to solve the stochastic inverse problem.\n",
    "\n",
    "The compact (bounded, finite-dimensional) paramter space for this example is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lam_domain = np.array([[-900, 1500], [.07, .15], [.1, .2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose an initial sample type to seed the sampling chains, which in this case comes from using Latin-Hypercube sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inital_sample_type = \"lhs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we adaptively generate the samples using `generalized_chains()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(my_disc, all_step_ratios) = sampler.generalized_chains(lam_domain,\n",
    "    transition_set, kernel_rD, sample_save_file, inital_sample_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Optional]:**\n",
    "\n",
    "We may choose to visualize the results by executing the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in points_ref and plot results\n",
    "ref_sample = mdat['points_true']\n",
    "ref_sample = ref_sample[:, 14]\n",
    "\n",
    "# Show the samples in the parameter space\n",
    "pDom.scatter_rhoD(my_disc, rho_D=rho_D, ref_sample=ref_sample, io_flag='input')\n",
    "# Show the corresponding samples in the data space\n",
    "pDom.scatter_rhoD(my_disc, rho_D=rho_D, ref_sample=Q_ref, io_flag='output')\n",
    "# Show the data domain that corresponds with the convex hull of samples in the\n",
    "# parameter space\n",
    "pDom.show_data_domain_2D(my_disc, Q_ref=Q_ref)\n",
    "\n",
    "# Show multiple data domains that correspond with the convex hull of samples in\n",
    "# the parameter space\n",
    "pDom.show_data_domain_multi(my_disc, Q_ref=Q_ref, showdim='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td colspan=3><center>Visualizations</center></td></tr><tr><td><img src='q1_q2_domain_Q_cs.png?1672295859'></td><td><img src='rhoD_samples_cs.png?1672295859'></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hack to refresh html after changes within notebook\n",
    "import random\n",
    "__counter__ = random.randint(0,2e9)\n",
    "\n",
    "# displays saved visualizations\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<table><tr><td colspan=3><center>Visualizations</center></td></tr>\"+\n",
    "             \"<tr><td><img src='q1_q2_domain_Q_cs.png?%d'></td>\"% __counter__+\n",
    "             \"<td><img src='rhoD_samples_cs.png?%d'></td></tr>\"% __counter__+\n",
    "             \"</table>\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    "> *The user could simply run the example [plotDomains3D.py](plotDomains3D.py) to see the results for a previously generated set of adaptive samples.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps (4)-(5) [user]: Defining and solving a stochastic inverse problem\n",
    "In the call to `sampler.generalized_chains` above, a discretization object is created and saved. The user may wish to follow some of the other examples (e.g., Example: Linear Map with Uniform Sampling or Example: Nonlinear Map with Uniform Sampling) along with the paper referenced above to describe a data distribution around a reference datum (Step (4)) and solve the stochastic inverse problem (Step (5)) using the adaptively generated discretization object by loading it from file. This can be done in a separate script (but do not forget to do Step (0) which sets up the environment before coding Steps (4) and (5))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and comparing several sets of adaptive samples\n",
    "In some instances the user may want to generate and compare several sets of adaptive samples using a surrogate model to determine what the best kernel, transition set, number of generalized chains, and chain length are before adaptively sampling a more computationally expensive model. See [sandbox_test_3D.py](sandbox_test_3D.py). The set up in sandbox_test_3D.py is very similar to the set up in [fromFile3D](fromFile3D.py) and is omitted for brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">*If all of the above code in this notebook has been run, our current environment is already set up to compare several sets of adaptive samples and we can proceed with the following exploration.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore several types of kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e1b963d28515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;31m# Run with varying kernels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m gen_results = sampler.run_gen(heur_list, rho_D, maximum, param_domain,\n\u001b[0;32m---> 18\u001b[0;31m         transition_set, sample_save_file) \n\u001b[0m",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\bet-2.0.0-py2.7.egg\\bet\\sampling\\adaptiveSampling.pyc\u001b[0m in \u001b[0;36mrun_gen\u001b[0;34m(self, kern_list, rho_D, maximum, input_domain, t_set, savefile, initial_sample_type, criterion)\u001b[0m\n\u001b[1;32m    268\u001b[0m             (discretization, step_sizes) = self.generalized_chains(\n\u001b[1;32m    269\u001b[0m                     \u001b[0minput_domain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msavefile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                     initial_sample_type, criterion)\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscretization\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mr_step_size\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\bet-2.0.0-py2.7.egg\\bet\\sampling\\adaptiveSampling.pyc\u001b[0m in \u001b[0;36mgeneralized_chains\u001b[0;34m(self, input_obj, t_set, kern, savefile, initial_sample_type, criterion, hot_start)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[1;31m# Solve the model for the input_new.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0moutput_new_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlb_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_values_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[1;31m# Make some decision about changing step_size(k).  There are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-69cf837e58f1>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         interp_values[:, i] = griddata(points.transpose(), Q[:, i],\n\u001b[0;32m---> 11\u001b[0;31m             inputs)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minterp_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\scipy\\interpolate\\ndgriddata.py\u001b[0m in \u001b[0;36mgriddata\u001b[0;34m(points, values, xi, method, fill_value, rescale)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         ip = LinearNDInterpolator(points, values, fill_value=fill_value,\n\u001b[0;32m--> 217\u001b[0;31m                                   rescale=rescale)\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cubic'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mscipy/interpolate/interpnd.pyx\u001b[0m in \u001b[0;36mscipy.interpolate.interpnd.LinearNDInterpolator.__init__ (scipy\\interpolate\\interpnd.c:4980)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mscipy/spatial/qhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull.Delaunay.__init__ (scipy\\spatial\\qhull.c:15948)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mscipy/spatial/qhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull._QhullUser.__init__ (scipy\\spatial\\qhull.c:14264)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mscipy/spatial/qhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull.Delaunay._update (scipy\\spatial\\qhull.c:16387)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mscipy/spatial/qhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull._QhullUser._update (scipy\\spatial\\qhull.c:14852)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amin\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kernel_mm = asam.maxima_mean_kernel(np.array([Q_ref]), rho_D)\n",
    "kernel_rD = asam.rhoD_kernel(maximum, rho_D)\n",
    "kernel_m = asam.maxima_kernel(np.array([Q_ref]), rho_D)\n",
    "heur_list = [kernel_mm, kernel_rD, kernel_m]\n",
    "\n",
    "\n",
    "# Set minima and maxima\n",
    "param_domain = np.array([[-900, 1500], [.07, .15], [.1, .2]])\n",
    "lam3 = 0.012\n",
    "xmin = 1420\n",
    "xmax = 1580\n",
    "ymax = 1500\n",
    "wall_height = -2.5\n",
    "\n",
    "# Get samples\n",
    "# Run with varying kernels\n",
    "gen_results = sampler.run_gen(heur_list, rho_D, maximum, param_domain,\n",
    "        transition_set, sample_save_file) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore `transition_set` with various inital, minimum, and maximum step size ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bbbf626e98aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m tk_results = sampler.run_tk(init_ratio, min_ratio, max_ratio, rho_D,\n\u001b[0;32m----> 6\u001b[0;31m         maximum, param_domain, kernel_rD, sample_save_file)\n\u001b[0m",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\bet-2.0.0-py2.7.egg\\bet\\sampling\\adaptiveSampling.pyc\u001b[0m in \u001b[0;36mrun_tk\u001b[0;34m(self, init_ratio, min_ratio, max_ratio, rho_D, maximum, input_domain, kernel, savefile, initial_sample_type, criterion)\u001b[0m\n\u001b[1;32m    319\u001b[0m             (discretization, step_sizes) = self.generalized_chains(\n\u001b[1;32m    320\u001b[0m                    \u001b[0minput_domain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msavefile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                     initial_sample_type, criterion)\n\u001b[0m\u001b[1;32m    322\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscretization\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mr_step_size\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\bet-2.0.0-py2.7.egg\\bet\\sampling\\adaptiveSampling.pyc\u001b[0m in \u001b[0;36mgeneralized_chains\u001b[0;34m(self, input_obj, t_set, kern, savefile, initial_sample_type, criterion, hot_start)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[1;31m# Solve the model for the input_new.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0moutput_new_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlb_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_values_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[1;31m# Make some decision about changing step_size(k).  There are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-69cf837e58f1>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         interp_values[:, i] = griddata(points.transpose(), Q[:, i],\n\u001b[0;32m---> 11\u001b[0;31m             inputs)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minterp_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\scipy\\interpolate\\ndgriddata.py\u001b[0m in \u001b[0;36mgriddata\u001b[0;34m(points, values, xi, method, fill_value, rescale)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'linear'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         ip = LinearNDInterpolator(points, values, fill_value=fill_value,\n\u001b[0;32m--> 217\u001b[0;31m                                   rescale=rescale)\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cubic'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mscipy/interpolate/interpnd.pyx\u001b[0m in \u001b[0;36mscipy.interpolate.interpnd.LinearNDInterpolator.__init__ (scipy\\interpolate\\interpnd.c:4980)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mscipy/spatial/qhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull.Delaunay.__init__ (scipy\\spatial\\qhull.c:15948)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mscipy/spatial/qhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull._QhullUser.__init__ (scipy\\spatial\\qhull.c:14264)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mscipy/spatial/qhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull.Delaunay._update (scipy\\spatial\\qhull.c:16387)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mscipy/spatial/qhull.pyx\u001b[0m in \u001b[0;36mscipy.spatial.qhull._QhullUser._update (scipy\\spatial\\qhull.c:14852)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amin\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run with varying transition sets bounds\n",
    "init_ratio = [0.1, 0.25, 0.5]\n",
    "min_ratio = [2e-3, 2e-5, 2e-8]\n",
    "max_ratio = [.5, .75, 1.0]\n",
    "tk_results = sampler.run_tk(init_ratio, min_ratio, max_ratio, rho_D,\n",
    "        maximum, param_domain, kernel_rD, sample_save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore a single kernel with varying values of ratios for increasing and decreasing the step size (i.e. the size of the hyperrectangle to draw a new step from using a transition set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "increase = [1.0, 2.0, 4.0]\n",
    "decrease = [0.5, 0.5e2, 0.5e3]\n",
    "tolerance = [1e-4, 1e-6, 1e-8]\n",
    "incdec_results = sampler.run_inc_dec(increase, decrease, tolerance, rho_D,\n",
    "        maximum, param_domain, transition_set, sample_save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    "> *The above examples just use a zip combination of the lists uses to define varying parameters for the kernels and transition sets. To explore the product of these lists you need to use numpy.meshgrid and numpy.ravel or a similar process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** *To compare the results in terms of yield or the total number of samples generated in the region of interest we could use `compare_yield` to display the results to screen, however this tool is being depreciated. Here compare_yield() simply would display to screen the `sample_quality` and `run_param` sorted by `sample_quality` and indexed by `sort_ind`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bet.postProcess.postTools as ptools\n",
    "# Compare the quality of several sets of samples\n",
    "print \"Compare yield of sample sets with various kernels\"\n",
    "ptools.compare_yield(gen_results[3], gen_results[2], gen_results[4])\n",
    "print \"Compare yield of sample sets with various transition sets bounds\"\n",
    "ptools.compare_yield(tk_results[3], tk_results[2], tk_results[4])\n",
    "print \"Compare yield of sample sets with variouos increase/decrease ratios\"\n",
    "ptools.compare_yield(incdec_results[3], incdec_results[2],incdec_results[4])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
