{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: FEniCS with Docker\n",
    "[Adapted from BET Documentation](http://ut-chg.github.io/BET/examples/example_rst_files/FEniCS.html#fenicsexample)\n",
    "\n",
    "We will walk through the following example. This example will only run in serial using serial runs of a model (described below). If the user takes Steps (0)-(3) and runs them in a separate script, then the saved discretization object can be loaded into a different script containing Steps (4)-(5) (and optionally including Step (6)) to solve the stochastic inverse problem in parallel using BET. To see an example describing how to run multiple instances of the (serial) model with different parameters, see Example: Multiple Serial FEniCS for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example requires the following external packages not shipped with BET:\n",
    "\n",
    "* An installation of Docker\n",
    "* A pulled image of FEniCS via Docker\n",
    "\n",
    "For more information on how to install docker on you computer, visit [the Docker website](https://www.docker.com/). Instructions on how to pull a working version of FEniCS via Docker can be [found here](http://fenics.readthedocs.io/projects/containers/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example generates samples for a KL expansion associated with a covariance defined by `cov` in computeSaveKL.py on an L-shaped mesh that defines the permeability field for a Poisson equation solved in myModel.py.\n",
    "\n",
    "The quantities of interest (QoI) are defined as two spatial averages of the solution to the PDE.\n",
    "\n",
    "The user defines the dimension of the parameter space (corresponding to the number of KL terms) and the number of samples in this space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Docker allows BET to utilize the FEniCS PDE solver without having to deal with several of the more arduous customizations required to set up FEniCS on the user's personal machine. In myModel_Interface.py, a Docker container is created and then all the FEniCS calculations are executed in the container, afterwhich the container is closed. Because of the portability and customizability of Docker containers, this example provides a framework for how BET could potentially be linked to all sorts of other computational models which require specific computational software.\n",
    "\n",
    "![Placeholder for actual picture visualizing BET / Docker / FEniCS container interaction](temp_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we are coupling to the state-of-the-art FEniCS code for solving a PDE, we again see that the actual process for solving the stochastic inverse problem is quite simple requiring a total of 5 steps with BET excluding any post-processing the user may want. In general the user will probably not write code with various options as was done here for pedagogical purposes. We break down the actual example included with BET step-by-step below, but first, to showcase the overall simplicitly, we show the “entire” code (omitting setting the environment, post-processing, and commenting) required for solving the stochastic inverse problem using some default options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "sampler = bsam.sampler(my_model)\n",
    "\n",
    "num_KL_terms = 2\n",
    "computeSaveKL(num_KL_terms)\n",
    "input_samples = samp.sample_set(num_KL_terms)\n",
    "KL_term_min = -3.0\n",
    "KL_term_max = 3.0\n",
    "input_samples.set_domain(np.repeat([[KL_term_min, KL_term_max]],\n",
    "                                   num_KL_terms,\n",
    "                                   axis=0))\n",
    "input_samples = sampler.regular_sample_set(input_samples, num_samples_per_dim=[10, 10])\n",
    "input_samples.estimate_volume_mc()\n",
    "\n",
    "my_discretization = sampler.compute_QoI_and_create_discretization(input_samples)\n",
    "\n",
    "param_ref = np.ones((1,num_KL_terms))\n",
    "Q_ref = my_model(param_ref)\n",
    "simpleFunP.regular_partition_uniform_distribution_rectangle_scaled(\n",
    "    data_set=my_discretization, Q_ref=Q_ref[0,:], rect_scale=0.1,\n",
    "    cells_per_dimension=3)\n",
    "\n",
    "calculateP.prob(my_discretization)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step (0): Setting up the environment\n",
    "Import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bet.calculateP.simpleFunP as simpleFunP\n",
    "import bet.calculateP.calculateP as calculateP\n",
    "import bet.postProcess.plotP as plotP\n",
    "import bet.postProcess.plotDomains as plotD\n",
    "import bet.sample as samp\n",
    "import bet.sampling.basicSampling as bsam\n",
    "import subprocess as sp\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step (1): Define interface to the model\n",
    "Import the Python script interface to the model using FEniCS that takes as input a numpy array of model input parameter samples, generated from the sampler (see below), creates or starts appropriate Docker container, and executes the FEniCS scripts to evaluate the model and generate QoI samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from myModel_Interface import bet_docker_interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the sampler that will be used to create the discretization object, which is the fundamental object used by BET to compute solutions to the stochastic inverse problem. The sampler and my_model is the interface of BET to the model, and it allows BET to create input/output samples of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = bsam.sampler(bet_docker_interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step (2): Describe and sample the input space\n",
    "We compute and save the KL expansion once so that this part, which can be computationally expensive, can be done just once and then commented out for future runs of the code using the same set of KL coefficients defining the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose the number of KL terms\n",
    "num_KL_terms = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Docker Container and Running KL Expansion\n",
    "\n",
    "Since the KL expansion requires FEniCS, we can create a FEniCS docker container to execute the KL expansion script. After we create the container, we can run the script in the container just once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following checks to make sure docker is installed and running on the user's computer by checking the Docker version. If it is not working, then it will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client:\n",
      " Version:      17.06.2-ce\n",
      " API version:  1.30\n",
      " Go version:   go1.8.3\n",
      " Git commit:   cec0b72\n",
      " Built:        Tue Sep  5 19:57:19 2017\n",
      " OS/Arch:      windows/amd64\n",
      "\n",
      "Server:\n",
      " Version:      17.06.2-ce\n",
      " API version:  1.30 (minimum version 1.12)\n",
      " Go version:   go1.8.3\n",
      " Git commit:   cec0b72\n",
      " Built:        Tue Sep  5 19:59:19 2017\n",
      " OS/Arch:      linux/amd64\n",
      " Experimental: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checks to make sure docker is installed and running correctly\n",
    "try:\n",
    "    dockercommand = (\"docker version\")\n",
    "    print(sp.check_output(dockercommand.split(\" \")))\n",
    "except:\n",
    "    print(\"\\n Error Running Docker: \\n Check that Docker \"+\n",
    "          \"is properly installed and currently running \\n\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the FEniCS Docker container. It shares the local directory as a \"volume\" with the container so that scripts in the current directory can be run inside the container and the results saved on the local machine.  For details about the options and arguments, see the [Docker documentation](https://docs.docker.com/engine/reference/commandline/run/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: on Windows, make sure appropriate drives have been shared via Docker for Windows settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get working directory and define container name\n",
    "localdirect = os.getcwd()\n",
    "containername = (\"ComputeKL_FEniCS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker create command string\n",
    "dockercreate = (\"docker create -i --name \"+containername\n",
    "                        + \" -w /home/fenics/shared\" # sets working directory\n",
    "                 +\" -v \"+localdirect+\":/home/fenics/shared\" # share current dir.\n",
    "                  +\" quay.io/fenicsproject/stable\") # name of parent image \n",
    "\n",
    "#print(dockercreate)\n",
    "\n",
    "# use subprocess to run command string and check output\n",
    "out = sp.check_output(dockercreate.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we start the container and execute the script [Compute_Save_KL.py](Compute_Save_KL.py). Note we also add the optional argument \"num_KL_terms\" to the script execution. The last few lines stops the container. \n",
    "\n",
    "*Note: Removing the container is optional, but note that you cannot create two containers with the same name.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComputeKL_FEniCS\n",
      " container has started...\n"
     ]
    }
   ],
   "source": [
    "# name of the python script which runs the fenics model\n",
    "fenics_script = (\"Compute_Save_KL.py \"+str(num_KL_terms))\n",
    "    \n",
    "# starts container\n",
    "dockerstart = (\"docker start \"+containername)\n",
    "outstatus = sp.check_output(dockerstart.split(\" \"))\n",
    "print(outstatus+\" container has started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "which is type: <type 'int'>\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling DOLFIN just-in-time (JIT) compiler, this may take some time.\n",
      "---------------------------\n",
      "---------------------------\n",
      " Building Covariance Matrix\n",
      "---------------------------\n",
      "---------------------------\n",
      "---------------------------\n",
      "---------------------------\n",
      " Finished Covariance Matrix\n",
      "---------------------------\n",
      "---------------------------\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "---------------------------\n",
      "---------------------------\n",
      " Building Mass Matrix \n",
      "---------------------------\n",
      "---------------------------\n",
      "---------------------------\n",
      "---------------------------\n",
      " Finished Mass Matrix \n",
      "---------------------------\n",
      "---------------------------\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "  Ignoring precision in integral metadata compiled using quadrature representation. Not implemented.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "Were the KL_expansions saved?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# execute python script in FEniCS container\n",
    "dockerexec = (\"docker exec \"+containername+\" python \"+fenics_script)\n",
    "\n",
    "outstatus = sp.Popen(dockerexec.split(\" \"),stdout=sp.PIPE)\n",
    "print(outstatus.communicate()[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComputeKL_FEniCS\n",
      " container has closed.\n"
     ]
    }
   ],
   "source": [
    "# close docker container\n",
    "dockerclose = (\"docker stop \"+containername)\n",
    "outstatus = sp.check_output(dockerclose.split(\" \"))\n",
    "print(outstatus+\" container has closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove docker container if needed\n",
    "#dockerRemove = (\"docker rm \"+containername)\n",
    "#outstatus = sp.check_output(dockerRemove.split(\" \"))\n",
    "#print(outstatus+\" has been removed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL Expansion should now be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Sampler\n",
    "Now we can initialize the parameter space and assume that any KL coefficient belongs to the interval [-3.0,3.0]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_samples = samp.sample_set(num_KL_terms)\n",
    "KL_term_min = -3.0\n",
    "KL_term_max = 3.0\n",
    "input_samples.set_domain(np.repeat([[KL_term_min, KL_term_max]],\n",
    "                               num_KL_terms,\n",
    "                               axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested changes for user exploration (1):\n",
    "Try with and without random sampling.\n",
    "\n",
    "If using **regular** sampling, try different numbers of samples per dimension (note that if `num_KL_terms` is not equal to 2, then the user needs to be careful using regular sampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randomSampling = False\n",
    "if randomSampling is True:\n",
    "    input_samples = sampler.random_sample_set('random', input_samples, \n",
    "                                              num_samples=1E2)\n",
    "else:\n",
    "    input_samples = sampler.regular_sample_set(input_samples, \n",
    "                                            num_samples_per_dim=[10, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested changes for user exploration (2):\n",
    "A standard Monte Carlo (MC) assumption is that every Voronoi cell has the same volume. If a regular grid of samples was used, then the standard MC assumption is true.\n",
    "\n",
    "See what happens if the MC assumption is not assumed to be true, and if different numbers of points are used to estimate the volumes of the Voronoi cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MC_assumption = True\n",
    "if MC_assumption is False:\n",
    "    input_samples.estimate_volume(n_mc_points=1E5)\n",
    "else:\n",
    "    input_samples.estimate_volume_mc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step (3): Generate QoI samples\n",
    "Create the discretization object holding all the input (parameter) samples and output (QoI) samples using the sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container named 'BET_to_FEniCS' not found. Creating new Docker container...\n",
      "New container created named: BET_to_FEniCS\n",
      "BET_to_FEniCS\n",
      " container has started...\n",
      "Calling FFC just-in-time (JIT) compiler, this may take some time.\n",
      "\n",
      "BET_to_FEniCS\n",
      " container has closed.\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'QoI_outsample_values.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-db720cdfd89b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m my_discretization = sampler.compute_QoI_and_create_discretization(\n\u001b[1;32m----> 2\u001b[1;33m                             input_samples, savefile='FEniCS_Example.txt.gz')\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\yent\\AppData\\Local\\Continuum\\Anaconda3\\envs\\BETenv2.7\\lib\\site-packages\\bet-2.0.0-py2.7.egg\\bet\\sampling\\basicSampling.pyc\u001b[0m in \u001b[0;36mcompute_QoI_and_create_discretization\u001b[1;34m(self, input_sample_set, savefile, globalize)\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[0minput_sample_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_to_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         local_output = self.lb_model(\\\n\u001b[1;32m--> 383\u001b[1;33m                 input_sample_set.get_values_local())\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[0mlocal_output_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yent\\Documents\\troy\\BET\\examples\\FEniCS\\myModel_Interface.py\u001b[0m in \u001b[0;36mbet_docker_interface\u001b[1;34m(parameter_samples)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m# Load and save the quantity of interest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mQoI_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"QoI_outsample_values.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mQoI_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\yent\\AppData\\Local\\Continuum\\Anaconda3\\envs\\BETenv2.7\\lib\\site-packages\\numpy\\lib\\npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'QoI_outsample_values.npy'"
     ]
    }
   ],
   "source": [
    "my_discretization = sampler.compute_QoI_and_create_discretization(\n",
    "                            input_samples, savefile='FEniCS_Example.txt.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, all of the model information has been extracted for BET (with the possibly exception of evaluating the model to generate a reference QoI datum or a distribution of the QoI), so the model is no longer required for evaluation. The user could do Steps (0)-(3) in a separate script, and then simply load the discretization object as part of a separate BET script that does the remaining steps. When the model is expensive to evaluate, this is an attractive option since we can now solve the stochastic inverse problem (with many different distributions defined on the data space) without ever having to re-solve the model (so long as we are happy with the resolution provided by the current discretization of the parameter and data spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_betenv2.7)",
   "language": "python",
   "name": "conda_betenv2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
