.. _linear:


===========================
Example: Linear Map Choose QoIs (volume / bin_ratio)
===========================

.. warning::

    The documentation for this example is NOT up to date. However, the code in
    the ``examples/`` directory is up to date.

This example generates uniform random samples in the unit hypercube and
corresponding QoIs (data) generated by a linear map Q.  We then calculate the
gradients using an RBF scheme and use the gradient information to choose the
optimal set of 2 (3, 4, ... Lambda_dim) QoIs to use in the inverse problem.

Every real world problem requires special attention regarding how we choose
*optimal QoIs*.  This set of examples (examples/sensitivity/linear) covers
some of the more common scenarios using easy to understand linear maps.

In this *volume_binratio* example we choose *optimal QoIs* to be the set of QoIs
of size Lambda_dim that produces the smallest support of the inverse solution,
assuming we define the uncertainty in our data relative to the range of data
measured in each QoI (bin_ratio).

Import the necessary modules::


    import numpy as np
    import bet.sensitivity.gradients as grad
    import bet.sensitivity.chooseQoIs as cQoI
    import bet.calculateP.simpleFunP as simpleFunP
    import bet.calculateP.calculateP as calculateP
    import bet.postProcess.postTools as postTools
    import bet.Comm as comm

Let Lambda be a 5 dimensional hypercube::

    Lambda_dim = 5
    Data_dim = 10
    num_samples = 1E5
    num_centers = 10

Let the map Q be a random matrix of size (Data_dim, Lambda_dim)::

    np.random.seed(0)
    Q = np.random.random([Data_dim, Lambda_dim])

Choose random samples in parameter space to solve the model::

    samples = np.random.random([num_samples, Lambda_dim])
    data = Q.dot(samples.transpose()).transpose()

Calculate the gradient vectors at some subset of the samples.  Here the
*normalize* argument is set to *True* because we are using *bin_ratio* to
determine the uncertainty in our data::

    G = grad.calculate_gradients_rbf(samples, data,
        centers=samples[:num_centers, :], normalize=True)

With these gradient vectors, we are now ready to choose an optimal set of
QoIs to use in the inverse problem, based on minimizing the support of the
inverse solution (volume).  The most robust method for this is
:meth:~bet.sensitivity.chooseQoIs.chooseOptQoIs_large which returns the
best set of 2, 3, 4 ... until Lambda_dim.  This method returns a list of
matrices.  Each matrix has 10 rows, the first column representing the
expected inverse volume ratio, and the rest of the columns the corresponding
QoI indices::

    best_sets = cQoI.chooseOptQoIs_large(G, volume=True)

At this point we have determined the optimal set of QoIs to use in the inverse
problem.  Now we compare the support of the inverse solution using
different sets of these QoIs.  We set Q_ref to correspond to the center of
the parameter space.  We choose the set of QoIs to consider::

    QoI_indices = [3, 6]
    #QoI_indices = [3, 4, 5, 8, 9]
    #QoI_indices = [2, 3, 6, 8, 9]

In this linear case we expect our ordering of sets of QoIs to be very good.  But
we see in this example that the set [3, 4, 5, 8, 9] (set 1) has a smaller
expected volume ratio than the set [2, 3, 6, 8, 9] (set 2), however the inverse
solution yields larger volume of support for set 1 than set 2.  This is likely
due to the fact that we restrict ourselves to the parameter space [0, 1]^5, and
the actual support of the inverse solution may extend out of this space.  The
expected volume ratio is computed assuming an unbounded parameter space.

Restrict the data to have just QoI_indices::

    data = data[:, QoI_indices]
    Q_ref = Q[QoI_indices, :].dot(0.5 * np.ones(Lambda_dim))

bin_ratio defines the uncertainty in our data::

    bin_ratio = 0.25

Find the simple function approximation::

    (P,  lam_vol, io_ptr) = calculateP.prob(samples=samples, data=data,
        rho_D_M=d_distr_prob, d_distr_samples=d_distr_samples)

Sort samples by highest probability density and find how many samples lie in
the support of the inverse solution.  With the Monte Carlo assumption, this
also tells us the approximate volume of this support::

    percentile = 1.0
    (num_samples, P_high, samples_high, lam_vol_high, data_high) =\
        postTools.sample_highest_prob(top_percentile=percentile, P_samples=P,
        samples=samples, lam_vol=lam_vol,data = data,sort=True)

Print the number of samples that make up the highest percentile percent
samples and ratio of the volume of the parameter domain they take up
::

    if comm.rank == 0:
        print (num_samples, np.sum(lam_vol_high))
